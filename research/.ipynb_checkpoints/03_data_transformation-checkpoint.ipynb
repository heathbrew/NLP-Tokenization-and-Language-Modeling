{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bf15b99-e25b-47cb-b22c-66fdfdcd149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36f70d52-9ecb-4991-953b-4e936e6cd5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Desktop\\\\NLP\\\\Lab 1\\\\NLP-Tokenization-and-Language-Modeling\\\\research'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee595830-35da-4eba-89c3-4e5d43cf18b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53cf3a64-a584-4c04-9ba4-80171a3b8da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Desktop\\\\NLP\\\\Lab 1\\\\NLP-Tokenization-and-Language-Modeling'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab8b077e-9d88-4de6-95d2-4e306030f63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-31 21:59:01,949 - INFO - Splitted data into training and test sets\n",
      "2024-01-31 21:59:01,953 - INFO - Training set size: 5414\n",
      "2024-01-31 21:59:01,956 - INFO - Test set size: 1805\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    train_file: Path\n",
    "    test_file: Path\n",
    "    dataset_file: Path\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self):\n",
    "        self.root_dir = Path(\"D:/Desktop/NLP/Lab 1/NLP-Tokenization-and-Language-Modeling\")\n",
    "        self.train_file = self.root_dir / \"dataset/UnzippedAngular/train.txt\"\n",
    "        self.test_file = self.root_dir / \"dataset/UnzippedAngular/test.txt\"\n",
    "        self.dataset_file = self.root_dir / \"dataset/UnzippedAngular/Angular Resume.txt\"\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        return DataTransformationConfig(\n",
    "            root_dir=self.root_dir,\n",
    "            train_file=self.train_file,\n",
    "            test_file=self.test_file,\n",
    "            dataset_file=self.dataset_file\n",
    "        )\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def spacy_tokenize(self, text):\n",
    "        doc = self.nlp(text)\n",
    "        return [token.text.lower() for token in doc if not token.is_punct and not token.is_space]\n",
    "\n",
    "    def train_test_splitting(self):\n",
    "        with open(self.config.dataset_file, 'r', encoding='utf-8') as file:\n",
    "            dataset_text = file.read()\n",
    "\n",
    "        # Tokenize the dataset text\n",
    "        tokenized_data = self.spacy_tokenize(dataset_text)\n",
    "\n",
    "        # Split the data into training and test sets (0.75, 0.25 split)\n",
    "        train, test = train_test_split(tokenized_data, test_size=0.25)\n",
    "\n",
    "        # Save the split data\n",
    "        with open(self.config.train_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(' '.join(train))\n",
    "\n",
    "        with open(self.config.test_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(' '.join(test))\n",
    "\n",
    "        logging.info(\"Splitted data into training and test sets\")\n",
    "        logging.info(f\"Training set size: {len(train)}\")\n",
    "        logging.info(f\"Test set size: {len(test)}\")\n",
    "\n",
    "def main():\n",
    "    config_manager = ConfigurationManager()\n",
    "    data_transformation_config = config_manager.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(data_transformation_config)\n",
    "\n",
    "    data_transformation.train_test_splitting()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "416e6d2a-4d42-49cc-8d2b-cbab0fcc9355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to 2021 description ‚óè professional limitless g input lmt pvt 2008 with kharage all new level differe\n"
     ]
    }
   ],
   "source": [
    "# Read the dataset from a text file\n",
    "file_path = \"dataset/UnzippedAngular/train.txt\"\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    dataset_text = file.read()\n",
    "\n",
    "print(dataset_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109aa9bd-f0fd-4e64-aa5c-4814f27330ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e4597e-1411-4ba7-ac25-e6c570e20b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
