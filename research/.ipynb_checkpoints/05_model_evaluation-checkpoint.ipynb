{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28acc7d6-2b7d-441b-9d43-64991e737927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fd92e91-636b-4552-a2ab-092d72edb624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Desktop\\\\NLP\\\\Lab 1\\\\NLP-Tokenization-and-Language-Modeling\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b00df3d-5443-45c1-b185-0f79fcec01dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17b8c4d1-2ab5-45b0-9a41-9408909137b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Desktop\\\\NLP\\\\Lab 1\\\\NLP-Tokenization-and-Language-Modeling'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a031bd6-efb3-4f27-834f-66430957e50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# import random\n",
    "# import joblib\n",
    "# from dataclasses import dataclass\n",
    "# from pathlib import Path\n",
    "# from collections import Counter, defaultdict\n",
    "# import spacy\n",
    "\n",
    "# # Configure logging\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# @dataclass(frozen=True)\n",
    "# class DataTransformationConfig:\n",
    "#     root_dir: Path\n",
    "#     test_file: Path\n",
    "#     unigram_model_path: Path\n",
    "#     bigram_model_path: Path\n",
    "\n",
    "# class ConfigurationManager:\n",
    "#     def __init__(self):\n",
    "#         self.root_dir = Path(\"D:/Desktop/NLP/Lab 1/NLP-Tokenization-and-Language-Modeling\")\n",
    "#         self.test_file = self.root_dir / \"dataset/UnzippedAngular/test.txt\"\n",
    "#         self.unigram_model_path = self.root_dir / \"Models/unigrammodel.pkl\"\n",
    "#         self.bigram_model_path = self.root_dir / \"Models/bigrammodel.pkl\"\n",
    "\n",
    "#     def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "#         return DataTransformationConfig(\n",
    "#             root_dir=self.root_dir,\n",
    "#             test_file=self.test_file,\n",
    "#             unigram_model_path=self.unigram_model_path,\n",
    "#             bigram_model_path=self.bigram_model_path\n",
    "#         )\n",
    "\n",
    "# def load_model(file_path):\n",
    "#     return joblib.load(file_path)\n",
    "\n",
    "# # Initialize spaCy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# def tokenize(text):\n",
    "#     doc = nlp(text)\n",
    "#     return [token.text.lower() for token in doc if not token.is_punct and not token.is_space]\n",
    "\n",
    "# def generate_sentence(start_word, unigram_model, bigram_model, length=15):\n",
    "#     sentence = [start_word]\n",
    "#     while len(sentence) < length:\n",
    "#         current_word = sentence[-1]\n",
    "#         next_words = bigram_model[current_word]\n",
    "\n",
    "#         if not next_words:\n",
    "#             next_word = random.choice(list(unigram_model.keys()))\n",
    "#         else:\n",
    "#             total_bigrams = sum(next_words.values())\n",
    "#             probs = [count / total_bigrams for count in next_words.values()]\n",
    "#             next_word = random.choices(list(next_words.keys()), weights=probs)[0]\n",
    "\n",
    "#         sentence.append(next_word)\n",
    "\n",
    "#     return ' '.join(sentence)\n",
    "\n",
    "# class ModelEvaluation:\n",
    "#     def __init__(self, config: DataTransformationConfig):\n",
    "#         self.config = config\n",
    "#         self.unigram_model = load_model(config.unigram_model_path)\n",
    "#         self.bigram_model = load_model(config.bigram_model_path)\n",
    "\n",
    "#     def generate_sentences_from_test(self,wordlength, num_sentences=5):\n",
    "#         with open(self.config.test_file, 'r', encoding='utf-8') as file:\n",
    "#             test_text = file.read()\n",
    "\n",
    "#         test_tokens = tokenize(test_text.lower())\n",
    "#         random_start_words = random.sample(test_tokens, num_sentences)\n",
    "\n",
    "#         for start_word in random_start_words:\n",
    "#             sentence = generate_sentence(start_word, self.unigram_model, self.bigram_model,wordlength)\n",
    "#             logging.info(f\"Generated sentence using Unigram Bigram Model with start word '{start_word}': {sentence}\")\n",
    "\n",
    "# import nltk\n",
    "# from nltk.translate.bleu_score import sentence_bleu\n",
    "# from rouge import Rouge\n",
    "# import math\n",
    "\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# def calculate_perplexity(sentence, unigram_model, bigram_model):\n",
    "#     words = sentence.split()\n",
    "#     perplexity = 1\n",
    "#     N = len(words)\n",
    "    \n",
    "#     for i in range(1, N):\n",
    "#         prev_word = words[i-1]\n",
    "#         word = words[i]\n",
    "#         probability = bigram_model[prev_word][word] / sum(bigram_model[prev_word].values())\n",
    "#         perplexity = perplexity * (1/probability)\n",
    "    \n",
    "#     perplexity = pow(perplexity, 1/float(N))\n",
    "#     return perplexity\n",
    "\n",
    "# def calculate_bleu_score(generated_sentence, reference_sentences):\n",
    "#     return sentence_bleu(reference_sentences, generated_sentence, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "# def calculate_rouge_score(generated_sentence, reference_sentences):\n",
    "#     rouge = Rouge()\n",
    "#     scores = rouge.get_scores(generated_sentence, reference_sentences)\n",
    "#     return scores[0]['rouge-1']['f']\n",
    "\n",
    "# # Example usage\n",
    "# generated_sentence = \"your generated sentence here\"\n",
    "# reference_sentences = [\"a list\", \"of reference\", \"sentences here\"]\n",
    "\n",
    "# perplexity = calculate_perplexity(generated_sentence, unigram_model, bigram_model)\n",
    "# bleu_score = calculate_bleu_score(nltk.word_tokenize(generated_sentence), [nltk.word_tokenize(ref) for ref in reference_sentences])\n",
    "# rouge_score = calculate_rouge_score(generated_sentence, ' '.join(reference_sentences))\n",
    "\n",
    "# print(f\"Perplexity: {perplexity}\")\n",
    "# print(f\"BLEU Score: {bleu_score}\")\n",
    "# print(f\"ROUGE Score: {rouge_score}\")\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     try:\n",
    "#         config_manager = ConfigurationManager()\n",
    "#         model_evaluation_config = config_manager.get_data_transformation_config()\n",
    "#         model_evaluation = ModelEvaluation(config=model_evaluation_config)\n",
    "        \n",
    "#         # Generate sentences using words from test.txt\n",
    "#         model_evaluation.generate_sentences_from_test(wordlength=20,num_sentences=6)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error occurred: {e}\")\n",
    "#         raise e\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7ff8cad-4984-41c1-8c3a-4affa1fbaa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge-score in d:\\desktop\\nlp\\lab 1\\nlp-tokenization-and-language-modeling\\venv\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in d:\\desktop\\nlp\\lab 1\\nlp-tokenization-and-language-modeling\\venv\\lib\\site-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: nltk in d:\\desktop\\nlp\\lab 1\\nlp-tokenization-and-language-modeling\\venv\\lib\\site-packages (from rouge-score) (3.8.1)\n",
      "Requirement already satisfied: numpy in d:\\desktop\\nlp\\lab 1\\nlp-tokenization-and-language-modeling\\venv\\lib\\site-packages (from rouge-score) (1.26.3)\n",
      "Requirement already satisfied: six>=1.14.0 in d:\\desktop\\nlp\\lab 1\\nlp-tokenization-and-language-modeling\\venv\\lib\\site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in d:\\desktop\\nlp\\lab 1\\nlp-tokenization-and-language-modeling\\venv\\lib\\site-packages (from nltk->rouge-score) (8.1.7)\n",
      "Requirement already satisfied: joblib in d:\\desktop\\nlp\\lab 1\\nlp-tokenization-and-language-modeling\\venv\\lib\\site-packages (from nltk->rouge-score) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\desktop\\nlp\\lab 1\\nlp-tokenization-and-language-modeling\\venv\\lib\\site-packages (from nltk->rouge-score) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in d:\\desktop\\nlp\\lab 1\\nlp-tokenization-and-language-modeling\\venv\\lib\\site-packages (from nltk->rouge-score) (4.66.1)\n",
      "Requirement already satisfied: colorama in d:\\desktop\\nlp\\lab 1\\nlp-tokenization-and-language-modeling\\venv\\lib\\site-packages (from click->nltk->rouge-score) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "! pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a269a93f-bdce-4e8b-9d4f-d0b56acf49a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-01 01:09:57,157 - INFO - Word: punjab)-147202 , Generated sentence: punjab)-147202 boundaries angularjs working showing learn under webservices ‚óè ministry of on structure description write 2016 work time please experience win32 college previous ms etc science objective significant having inter\n",
      "2024-02-01 01:09:57,246 - INFO - Using default tokenizer.\n",
      "2024-02-01 01:09:57,254 - INFO - Perplexity: 6.164590785369434\n",
      "2024-02-01 01:09:57,256 - INFO - BLEU Score: 1.3091834502273125e-231\n",
      "2024-02-01 01:09:57,258 - INFO - ROUGE Score: 0.13333333333333333\n",
      "2024-02-01 01:09:57,261 - INFO - Word: fundamentals , Generated sentence: fundamentals for august training vendiman check worked created 3 oracle a hosting visually database all the independently scripting contribute responsibilities core can jan-2014 creation 2 ÔÇ∑ client deals modules an\n",
      "2024-02-01 01:09:57,351 - INFO - Using default tokenizer.\n",
      "2024-02-01 01:09:57,358 - INFO - Perplexity: 5.460364768198647\n",
      "2024-02-01 01:09:57,359 - INFO - BLEU Score: 7.784451369270533e-232\n",
      "2024-02-01 01:09:57,360 - INFO - ROUGE Score: 0.05\n",
      "2024-02-01 01:09:57,362 - INFO - Word: language , Generated sentence: language qualification and kit19 languages from new ltd ado.net +91 on up approach fixed skills skills skills for university coordination airtel for role modules 2017 system 1991 tools real net\n",
      "2024-02-01 01:09:57,444 - INFO - Using default tokenizer.\n",
      "2024-02-01 01:09:57,451 - INFO - Perplexity: 8.215985236874774\n",
      "2024-02-01 01:09:57,454 - INFO - BLEU Score: 1.2183324802375697e-231\n",
      "2024-02-01 01:09:57,455 - INFO - ROUGE Score: 0.1322314049586777\n",
      "2024-02-01 01:09:57,457 - INFO - Word: 2019 , Generated sentence: 2019 bls based site and which ajax role javascript debugger vending c deals false the sql tweets and 6 for working and mobile and name mobile and b.tech wordpress 4\n",
      "2024-02-01 01:09:57,551 - INFO - Using default tokenizer.\n",
      "2024-02-01 01:09:57,559 - INFO - Perplexity: 11.45916239499362\n",
      "2024-02-01 01:09:57,562 - INFO - BLEU Score: 1.4176733688536472e-231\n",
      "2024-02-01 01:09:57,563 - INFO - ROUGE Score: 0.19834710743801653\n",
      "2024-02-01 01:09:57,566 - INFO - Word: of , Generated sentence: of of india skill worked environment from plan institute angular technical muradnagar fair jawahar above sql 2017 the dikesh.ray@gmail.com by testing json service(using a take ‚Äôs project is budget it\n",
      "2024-02-01 01:09:57,658 - INFO - Using default tokenizer.\n",
      "2024-02-01 01:09:57,666 - INFO - Perplexity: 8.620514196488196\n",
      "2024-02-01 01:09:57,667 - INFO - BLEU Score: 1.1640469867513693e-231\n",
      "2024-02-01 01:09:57,668 - INFO - ROUGE Score: 0.14516129032258066\n",
      "2024-02-01 01:09:57,670 - INFO - Word: database , Generated sentence: database and shaheen indian time this on makes bachelor apis than fundamentals system at description paper years a microsoft queries ÔÇ∑ the global details level according e.g. singh sql clinic\n",
      "2024-02-01 01:09:57,767 - INFO - Using default tokenizer.\n",
      "2024-02-01 01:09:57,777 - INFO - Perplexity: 6.981906666741869\n",
      "2024-02-01 01:09:57,779 - INFO - BLEU Score: 1.1640469867513693e-231\n",
      "2024-02-01 01:09:57,780 - INFO - ROUGE Score: 0.08333333333333333\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import random\n",
    "import joblib\n",
    "import spacy\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    test_file: Path\n",
    "    unigram_model_path: Path\n",
    "    bigram_model_path: Path\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self):\n",
    "        self.root_dir = Path(\"D:/Desktop/NLP/Lab 1/NLP-Tokenization-and-Language-Modeling\")\n",
    "        self.test_file = self.root_dir / \"dataset/UnzippedAngular/test.txt\"\n",
    "        self.unigram_model_path = self.root_dir / \"Models/unigrammodel.pkl\"\n",
    "        self.bigram_model_path = self.root_dir / \"Models/bigrammodel.pkl\"\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        return DataTransformationConfig(\n",
    "            root_dir=self.root_dir,\n",
    "            test_file=self.test_file,\n",
    "            unigram_model_path=self.unigram_model_path,\n",
    "            bigram_model_path=self.bigram_model_path\n",
    "        )\n",
    "\n",
    "def load_model(file_path):\n",
    "    return joblib.load(file_path)\n",
    "\n",
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text.lower() for token in doc if not token.is_punct and not token.is_space]\n",
    "\n",
    "def generate_sentence(start_word, unigram_model, bigram_model, length=15):\n",
    "    sentence = [start_word]\n",
    "    while len(sentence) < length:\n",
    "        current_word = sentence[-1]\n",
    "        next_words = bigram_model[current_word]\n",
    "\n",
    "        if not next_words:\n",
    "            next_word = random.choice(list(unigram_model.keys()))\n",
    "        else:\n",
    "            total_bigrams = sum(next_words.values())\n",
    "            probs = [count / total_bigrams for count in next_words.values()]\n",
    "            next_word = random.choices(list(next_words.keys()), weights=probs)[0]\n",
    "\n",
    "        sentence.append(next_word)\n",
    "\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "def calculate_perplexity(sentence, unigram_model, bigram_model):\n",
    "    words = sentence.split()\n",
    "    perplexity = 1\n",
    "    N = len(words)\n",
    "    \n",
    "    for i in range(1, N):\n",
    "        prev_word = words[i-1]\n",
    "        word = words[i]\n",
    "        if prev_word in bigram_model and word in bigram_model[prev_word]:\n",
    "            probability = bigram_model[prev_word][word] / sum(bigram_model[prev_word].values())\n",
    "        else:\n",
    "            probability = 1 / len(unigram_model)\n",
    "        perplexity *= (1 / probability)\n",
    "    \n",
    "    perplexity = pow(perplexity, 1 / float(N))\n",
    "    return perplexity\n",
    "\n",
    "def calculate_bleu_score(generated_sentence, reference_sentences):\n",
    "    tokenized_generated_sentence = tokenize(generated_sentence)\n",
    "    tokenized_reference_sentences = [tokenize(ref) for ref in reference_sentences]\n",
    "    return sentence_bleu(tokenized_reference_sentences, tokenized_generated_sentence, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "def calculate_rouge_score(generated_sentence, reference_sentences):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference_sentences, generated_sentence)\n",
    "    return scores['rouge1'].fmeasure\n",
    "\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        self.unigram_model = load_model(config.unigram_model_path)\n",
    "        self.bigram_model = load_model(config.bigram_model_path)\n",
    "\n",
    "    def generate_sentences_from_test(self, Charlength=15, num_sentences=5):\n",
    "        with open(self.config.test_file, 'r', encoding='utf-8') as file:\n",
    "            test_text = file.read()\n",
    "\n",
    "        test_tokens = tokenize(test_text.lower())\n",
    "        random_start_words = random.sample(test_tokens, num_sentences)\n",
    "\n",
    "        for start_word in random_start_words:\n",
    "            sentence = generate_sentence(start_word, self.unigram_model, self.bigram_model, Charlength)\n",
    "            logging.info(f\"Word: {start_word} , Generated sentence: {sentence}\")\n",
    "\n",
    "            # Evaluation metrics\n",
    "            # Here you need to define your reference sentences\n",
    "            reference_sentences = [\"\"\"Having work experience in Information Technology with skills in  analysis, design, development, testing and deploying various software applications, with primary focus on developing software , solutions related to processing.\"\"\",\n",
    "                                   \"\"\"I am MCA degree holder having 5.4 years of experience in analyzing, designing and developing\n",
    "                                        web applications. At present, working with BLS international Services ltd. My primary involvement\n",
    "                                        is in developing web and windows application using C#, ADO.Net, ASP.Net, WCF, MVC, Web\n",
    "                                        Services, JQuery, Ajax, Json, AngularJS, Ms Azure and SQL Server and responsible for complete\n",
    "                                        implementation and testing\"\"\"]\n",
    "\n",
    "            perplexity = calculate_perplexity(sentence, self.unigram_model, self.bigram_model)\n",
    "            bleu_score = calculate_bleu_score(sentence, reference_sentences)\n",
    "            rouge_score = calculate_rouge_score(sentence, ' '.join(reference_sentences))\n",
    "\n",
    "            logging.info(f\"Perplexity: {perplexity}\")\n",
    "            logging.info(f\"BLEU Score: {bleu_score}\")\n",
    "            logging.info(f\"ROUGE Score: {rouge_score}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        config_manager = ConfigurationManager()\n",
    "        model_evaluation_config = config_manager.get_data_transformation_config()\n",
    "        model_evaluation = ModelEvaluation(config=model_evaluation_config)\n",
    "        \n",
    "        # Generate sentences and evaluate\n",
    "        model_evaluation.generate_sentences_from_test(Charlength=30, num_sentences=6)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error occurred: {e}\")\n",
    "        raise e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d732d9-31ef-46a8-8b1b-db06e5c0aef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
