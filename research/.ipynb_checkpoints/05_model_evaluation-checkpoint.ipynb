{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28acc7d6-2b7d-441b-9d43-64991e737927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fd92e91-636b-4552-a2ab-092d72edb624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Desktop\\\\NLP\\\\Lab 1\\\\NLP-Tokenization-and-Language-Modeling\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b00df3d-5443-45c1-b185-0f79fcec01dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17b8c4d1-2ab5-45b0-9a41-9408909137b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Desktop\\\\NLP\\\\Lab 1\\\\NLP-Tokenization-and-Language-Modeling'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a031bd6-efb3-4f27-834f-66430957e50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# import random\n",
    "# import joblib\n",
    "# from dataclasses import dataclass\n",
    "# from pathlib import Path\n",
    "# from collections import Counter, defaultdict\n",
    "# import spacy\n",
    "\n",
    "# # Configure logging\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# @dataclass(frozen=True)\n",
    "# class DataTransformationConfig:\n",
    "#     root_dir: Path\n",
    "#     test_file: Path\n",
    "#     unigram_model_path: Path\n",
    "#     bigram_model_path: Path\n",
    "\n",
    "# class ConfigurationManager:\n",
    "#     def __init__(self):\n",
    "#         self.root_dir = Path(\"D:/Desktop/NLP/Lab 1/NLP-Tokenization-and-Language-Modeling\")\n",
    "#         self.test_file = self.root_dir / \"dataset/UnzippedAngular/test.txt\"\n",
    "#         self.unigram_model_path = self.root_dir / \"Models/unigrammodel.pkl\"\n",
    "#         self.bigram_model_path = self.root_dir / \"Models/bigrammodel.pkl\"\n",
    "\n",
    "#     def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "#         return DataTransformationConfig(\n",
    "#             root_dir=self.root_dir,\n",
    "#             test_file=self.test_file,\n",
    "#             unigram_model_path=self.unigram_model_path,\n",
    "#             bigram_model_path=self.bigram_model_path\n",
    "#         )\n",
    "\n",
    "# def load_model(file_path):\n",
    "#     return joblib.load(file_path)\n",
    "\n",
    "# # Initialize spaCy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# def tokenize(text):\n",
    "#     doc = nlp(text)\n",
    "#     return [token.text.lower() for token in doc if not token.is_punct and not token.is_space]\n",
    "\n",
    "# def generate_sentence(start_word, unigram_model, bigram_model, length=15):\n",
    "#     sentence = [start_word]\n",
    "#     while len(sentence) < length:\n",
    "#         current_word = sentence[-1]\n",
    "#         next_words = bigram_model[current_word]\n",
    "\n",
    "#         if not next_words:\n",
    "#             next_word = random.choice(list(unigram_model.keys()))\n",
    "#         else:\n",
    "#             total_bigrams = sum(next_words.values())\n",
    "#             probs = [count / total_bigrams for count in next_words.values()]\n",
    "#             next_word = random.choices(list(next_words.keys()), weights=probs)[0]\n",
    "\n",
    "#         sentence.append(next_word)\n",
    "\n",
    "#     return ' '.join(sentence)\n",
    "\n",
    "# class ModelEvaluation:\n",
    "#     def __init__(self, config: DataTransformationConfig):\n",
    "#         self.config = config\n",
    "#         self.unigram_model = load_model(config.unigram_model_path)\n",
    "#         self.bigram_model = load_model(config.bigram_model_path)\n",
    "\n",
    "#     def generate_sentences_from_test(self,wordlength, num_sentences=5):\n",
    "#         with open(self.config.test_file, 'r', encoding='utf-8') as file:\n",
    "#             test_text = file.read()\n",
    "\n",
    "#         test_tokens = tokenize(test_text.lower())\n",
    "#         random_start_words = random.sample(test_tokens, num_sentences)\n",
    "\n",
    "#         for start_word in random_start_words:\n",
    "#             sentence = generate_sentence(start_word, self.unigram_model, self.bigram_model,wordlength)\n",
    "#             logging.info(f\"Generated sentence using Unigram Bigram Model with start word '{start_word}': {sentence}\")\n",
    "\n",
    "# import nltk\n",
    "# from nltk.translate.bleu_score import sentence_bleu\n",
    "# from rouge import Rouge\n",
    "# import math\n",
    "\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# def calculate_perplexity(sentence, unigram_model, bigram_model):\n",
    "#     words = sentence.split()\n",
    "#     perplexity = 1\n",
    "#     N = len(words)\n",
    "    \n",
    "#     for i in range(1, N):\n",
    "#         prev_word = words[i-1]\n",
    "#         word = words[i]\n",
    "#         probability = bigram_model[prev_word][word] / sum(bigram_model[prev_word].values())\n",
    "#         perplexity = perplexity * (1/probability)\n",
    "    \n",
    "#     perplexity = pow(perplexity, 1/float(N))\n",
    "#     return perplexity\n",
    "\n",
    "# def calculate_bleu_score(generated_sentence, reference_sentences):\n",
    "#     return sentence_bleu(reference_sentences, generated_sentence, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "# def calculate_rouge_score(generated_sentence, reference_sentences):\n",
    "#     rouge = Rouge()\n",
    "#     scores = rouge.get_scores(generated_sentence, reference_sentences)\n",
    "#     return scores[0]['rouge-1']['f']\n",
    "\n",
    "# # Example usage\n",
    "# generated_sentence = \"your generated sentence here\"\n",
    "# reference_sentences = [\"a list\", \"of reference\", \"sentences here\"]\n",
    "\n",
    "# perplexity = calculate_perplexity(generated_sentence, unigram_model, bigram_model)\n",
    "# bleu_score = calculate_bleu_score(nltk.word_tokenize(generated_sentence), [nltk.word_tokenize(ref) for ref in reference_sentences])\n",
    "# rouge_score = calculate_rouge_score(generated_sentence, ' '.join(reference_sentences))\n",
    "\n",
    "# print(f\"Perplexity: {perplexity}\")\n",
    "# print(f\"BLEU Score: {bleu_score}\")\n",
    "# print(f\"ROUGE Score: {rouge_score}\")\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     try:\n",
    "#         config_manager = ConfigurationManager()\n",
    "#         model_evaluation_config = config_manager.get_data_transformation_config()\n",
    "#         model_evaluation = ModelEvaluation(config=model_evaluation_config)\n",
    "        \n",
    "#         # Generate sentences using words from test.txt\n",
    "#         model_evaluation.generate_sentences_from_test(wordlength=20,num_sentences=6)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error occurred: {e}\")\n",
    "#         raise e\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7ff8cad-4984-41c1-8c3a-4affa1fbaa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge-score in d:\\desktop\\nlp\\lab 1\\nlp-tokenization-and-language-modeling\\venv\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in d:\\desktop\\nlp\\lab 1\\nlp-tokenization-and-language-modeling\\venv\\lib\\site-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: nltk in d:\\desktop\\nlp\\lab 1\\nlp-tokenization-and-language-modeling\\venv\\lib\\site-packages (from rouge-score) (3.8.1)\n",
      "Requirement already satisfied: numpy in d:\\desktop\\nlp\\lab 1\\nlp-tokenization-and-language-modeling\\venv\\lib\\site-packages (from rouge-score) (1.26.3)\n",
      "Requirement already satisfied: six>=1.14.0 in d:\\desktop\\nlp\\lab 1\\nlp-tokenization-and-language-modeling\\venv\\lib\\site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in d:\\desktop\\nlp\\lab 1\\nlp-tokenization-and-language-modeling\\venv\\lib\\site-packages (from nltk->rouge-score) (8.1.7)\n",
      "Requirement already satisfied: joblib in d:\\desktop\\nlp\\lab 1\\nlp-tokenization-and-language-modeling\\venv\\lib\\site-packages (from nltk->rouge-score) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\desktop\\nlp\\lab 1\\nlp-tokenization-and-language-modeling\\venv\\lib\\site-packages (from nltk->rouge-score) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in d:\\desktop\\nlp\\lab 1\\nlp-tokenization-and-language-modeling\\venv\\lib\\site-packages (from nltk->rouge-score) (4.66.1)\n",
      "Requirement already satisfied: colorama in d:\\desktop\\nlp\\lab 1\\nlp-tokenization-and-language-modeling\\venv\\lib\\site-packages (from click->nltk->rouge-score) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "! pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a269a93f-bdce-4e8b-9d4f-d0b56acf49a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-01 01:09:57,157 - INFO - Word: punjab)-147202 , Generated sentence: punjab)-147202 boundaries angularjs working showing learn under webservices ● ministry of on structure description write 2016 work time please experience win32 college previous ms etc science objective significant having inter\n",
      "2024-02-01 01:09:57,246 - INFO - Using default tokenizer.\n",
      "2024-02-01 01:09:57,254 - INFO - Perplexity: 6.164590785369434\n",
      "2024-02-01 01:09:57,256 - INFO - BLEU Score: 1.3091834502273125e-231\n",
      "2024-02-01 01:09:57,258 - INFO - ROUGE Score: 0.13333333333333333\n",
      "2024-02-01 01:09:57,261 - INFO - Word: fundamentals , Generated sentence: fundamentals for august training vendiman check worked created 3 oracle a hosting visually database all the independently scripting contribute responsibilities core can jan-2014 creation 2  client deals modules an\n",
      "2024-02-01 01:09:57,351 - INFO - Using default tokenizer.\n",
      "2024-02-01 01:09:57,358 - INFO - Perplexity: 5.460364768198647\n",
      "2024-02-01 01:09:57,359 - INFO - BLEU Score: 7.784451369270533e-232\n",
      "2024-02-01 01:09:57,360 - INFO - ROUGE Score: 0.05\n",
      "2024-02-01 01:09:57,362 - INFO - Word: language , Generated sentence: language qualification and kit19 languages from new ltd ado.net +91 on up approach fixed skills skills skills for university coordination airtel for role modules 2017 system 1991 tools real net\n",
      "2024-02-01 01:09:57,444 - INFO - Using default tokenizer.\n",
      "2024-02-01 01:09:57,451 - INFO - Perplexity: 8.215985236874774\n",
      "2024-02-01 01:09:57,454 - INFO - BLEU Score: 1.2183324802375697e-231\n",
      "2024-02-01 01:09:57,455 - INFO - ROUGE Score: 0.1322314049586777\n",
      "2024-02-01 01:09:57,457 - INFO - Word: 2019 , Generated sentence: 2019 bls based site and which ajax role javascript debugger vending c deals false the sql tweets and 6 for working and mobile and name mobile and b.tech wordpress 4\n",
      "2024-02-01 01:09:57,551 - INFO - Using default tokenizer.\n",
      "2024-02-01 01:09:57,559 - INFO - Perplexity: 11.45916239499362\n",
      "2024-02-01 01:09:57,562 - INFO - BLEU Score: 1.4176733688536472e-231\n",
      "2024-02-01 01:09:57,563 - INFO - ROUGE Score: 0.19834710743801653\n",
      "2024-02-01 01:09:57,566 - INFO - Word: of , Generated sentence: of of india skill worked environment from plan institute angular technical muradnagar fair jawahar above sql 2017 the dikesh.ray@gmail.com by testing json service(using a take ’s project is budget it\n",
      "2024-02-01 01:09:57,658 - INFO - Using default tokenizer.\n",
      "2024-02-01 01:09:57,666 - INFO - Perplexity: 8.620514196488196\n",
      "2024-02-01 01:09:57,667 - INFO - BLEU Score: 1.1640469867513693e-231\n",
      "2024-02-01 01:09:57,668 - INFO - ROUGE Score: 0.14516129032258066\n",
      "2024-02-01 01:09:57,670 - INFO - Word: database , Generated sentence: database and shaheen indian time this on makes bachelor apis than fundamentals system at description paper years a microsoft queries  the global details level according e.g. singh sql clinic\n",
      "2024-02-01 01:09:57,767 - INFO - Using default tokenizer.\n",
      "2024-02-01 01:09:57,777 - INFO - Perplexity: 6.981906666741869\n",
      "2024-02-01 01:09:57,779 - INFO - BLEU Score: 1.1640469867513693e-231\n",
      "2024-02-01 01:09:57,780 - INFO - ROUGE Score: 0.08333333333333333\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import random\n",
    "import joblib\n",
    "import spacy\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    test_file: Path\n",
    "    unigram_model_path: Path\n",
    "    bigram_model_path: Path\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self):\n",
    "        self.root_dir = Path(\"D:/Desktop/NLP/Lab 1/NLP-Tokenization-and-Language-Modeling\")\n",
    "        self.test_file = self.root_dir / \"dataset/UnzippedAngular/test.txt\"\n",
    "        self.unigram_model_path = self.root_dir / \"Models/unigrammodel.pkl\"\n",
    "        self.bigram_model_path = self.root_dir / \"Models/bigrammodel.pkl\"\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        return DataTransformationConfig(\n",
    "            root_dir=self.root_dir,\n",
    "            test_file=self.test_file,\n",
    "            unigram_model_path=self.unigram_model_path,\n",
    "            bigram_model_path=self.bigram_model_path\n",
    "        )\n",
    "\n",
    "def load_model(file_path):\n",
    "    return joblib.load(file_path)\n",
    "\n",
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text.lower() for token in doc if not token.is_punct and not token.is_space]\n",
    "\n",
    "def generate_sentence(start_word, unigram_model, bigram_model, length=15):\n",
    "    sentence = [start_word]\n",
    "    while len(sentence) < length:\n",
    "        current_word = sentence[-1]\n",
    "        next_words = bigram_model[current_word]\n",
    "\n",
    "        if not next_words:\n",
    "            next_word = random.choice(list(unigram_model.keys()))\n",
    "        else:\n",
    "            total_bigrams = sum(next_words.values())\n",
    "            probs = [count / total_bigrams for count in next_words.values()]\n",
    "            next_word = random.choices(list(next_words.keys()), weights=probs)[0]\n",
    "\n",
    "        sentence.append(next_word)\n",
    "\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "def calculate_perplexity(sentence, unigram_model, bigram_model):\n",
    "    words = sentence.split()\n",
    "    perplexity = 1\n",
    "    N = len(words)\n",
    "    \n",
    "    for i in range(1, N):\n",
    "        prev_word = words[i-1]\n",
    "        word = words[i]\n",
    "        if prev_word in bigram_model and word in bigram_model[prev_word]:\n",
    "            probability = bigram_model[prev_word][word] / sum(bigram_model[prev_word].values())\n",
    "        else:\n",
    "            probability = 1 / len(unigram_model)\n",
    "        perplexity *= (1 / probability)\n",
    "    \n",
    "    perplexity = pow(perplexity, 1 / float(N))\n",
    "    return perplexity\n",
    "\n",
    "def calculate_bleu_score(generated_sentence, reference_sentences):\n",
    "    tokenized_generated_sentence = tokenize(generated_sentence)\n",
    "    tokenized_reference_sentences = [tokenize(ref) for ref in reference_sentences]\n",
    "    return sentence_bleu(tokenized_reference_sentences, tokenized_generated_sentence, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "def calculate_rouge_score(generated_sentence, reference_sentences):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference_sentences, generated_sentence)\n",
    "    return scores['rouge1'].fmeasure\n",
    "\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        self.unigram_model = load_model(config.unigram_model_path)\n",
    "        self.bigram_model = load_model(config.bigram_model_path)\n",
    "\n",
    "    def generate_sentences_from_test(self, Charlength=15, num_sentences=5):\n",
    "        with open(self.config.test_file, 'r', encoding='utf-8') as file:\n",
    "            test_text = file.read()\n",
    "\n",
    "        test_tokens = tokenize(test_text.lower())\n",
    "        random_start_words = random.sample(test_tokens, num_sentences)\n",
    "\n",
    "        for start_word in random_start_words:\n",
    "            sentence = generate_sentence(start_word, self.unigram_model, self.bigram_model, Charlength)\n",
    "            logging.info(f\"Word: {start_word} , Generated sentence: {sentence}\")\n",
    "\n",
    "            # Evaluation metrics\n",
    "            # Here you need to define your reference sentences\n",
    "            reference_sentences = [\"\"\"Having work experience in Information Technology with skills in  analysis, design, development, testing and deploying various software applications, with primary focus on developing software , solutions related to processing.\"\"\",\n",
    "                                   \"\"\"I am MCA degree holder having 5.4 years of experience in analyzing, designing and developing\n",
    "                                        web applications. At present, working with BLS international Services ltd. My primary involvement\n",
    "                                        is in developing web and windows application using C#, ADO.Net, ASP.Net, WCF, MVC, Web\n",
    "                                        Services, JQuery, Ajax, Json, AngularJS, Ms Azure and SQL Server and responsible for complete\n",
    "                                        implementation and testing\"\"\"]\n",
    "\n",
    "            perplexity = calculate_perplexity(sentence, self.unigram_model, self.bigram_model)\n",
    "            bleu_score = calculate_bleu_score(sentence, reference_sentences)\n",
    "            rouge_score = calculate_rouge_score(sentence, ' '.join(reference_sentences))\n",
    "\n",
    "            logging.info(f\"Perplexity: {perplexity}\")\n",
    "            logging.info(f\"BLEU Score: {bleu_score}\")\n",
    "            logging.info(f\"ROUGE Score: {rouge_score}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        config_manager = ConfigurationManager()\n",
    "        model_evaluation_config = config_manager.get_data_transformation_config()\n",
    "        model_evaluation = ModelEvaluation(config=model_evaluation_config)\n",
    "        \n",
    "        # Generate sentences and evaluate\n",
    "        model_evaluation.generate_sentences_from_test(Charlength=30, num_sentences=6)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error occurred: {e}\")\n",
    "        raise e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d732d9-31ef-46a8-8b1b-db06e5c0aef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
