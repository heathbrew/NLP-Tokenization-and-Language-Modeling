{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30aea22b-84d8-4529-899e-a41dc40f119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "921ae979-43a6-48ae-b5a8-62807cb5a61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Desktop\\\\NLP\\\\Lab 1\\\\NLP-Tokenization-and-Language-Modeling\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f60a748d-09cf-42bb-a646-c2882c9f6773",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e41d8376-74fe-48a7-ad77-5cf2bea31638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Desktop\\\\NLP\\\\Lab 1\\\\NLP-Tokenization-and-Language-Modeling'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d39975f-5cc7-41ed-a8a8-9d447de1a114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated text saved to dataset/UnzippedAngular/Angular Resume.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "from docx import Document\n",
    "\n",
    "# Function to read PDF file and return text\n",
    "def read_pdf(pdf_file):\n",
    "    pdf_text = ''\n",
    "    pdf_reader = PdfReader(pdf_file)\n",
    "    for page in pdf_reader.pages:\n",
    "        pdf_text += page.extract_text()\n",
    "    return pdf_text\n",
    "\n",
    "# Function to read DOCX file and return text\n",
    "def read_docx(docx_file):\n",
    "    doc = Document(docx_file)\n",
    "    doc_text = ''\n",
    "    for paragraph in doc.paragraphs:\n",
    "        doc_text += paragraph.text + '\\n'\n",
    "    return doc_text\n",
    "\n",
    "# Main function to concatenate PDF and DOCX files in a folder\n",
    "def concatenate_files_in_folder(folder_name):\n",
    "    concatenated_text = ''\n",
    "    for filename in os.listdir(folder_name):\n",
    "        file_path = os.path.join(folder_name, filename)\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_text = read_pdf(file_path)\n",
    "            concatenated_text += pdf_text + '\\n\\n'\n",
    "        elif filename.endswith('.docx'):\n",
    "            docx_text = read_docx(file_path)\n",
    "            concatenated_text += docx_text + '\\n\\n'\n",
    "    return concatenated_text\n",
    "\n",
    "\n",
    "# Function to write text to a new text file\n",
    "def write_to_text_file(text, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "\n",
    "# Example usage:\n",
    "folder_name = 'dataset/UnzippedAngular'\n",
    "concatenated_text = concatenate_files_in_folder(folder_name)\n",
    "output_file = 'dataset/UnzippedAngular/Angular Resume.txt'\n",
    "write_to_text_file(concatenated_text, output_file)\n",
    "print(f'Concatenated text saved to {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dc914a3-17d1-412b-92bd-02cfe0650131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dikesh Ray Mobile: 7827137507 Email: Dikesh.ray@gmail.com Subject: Cover Letter – Software Developer Dear Sir/Madam, I am MCA degree holder having 5.4 years of experience in analyzing, designing and developing web applications. At present, working with BLS international Services ltd. My primary involvement is in developing web and windows application using C#, ADO.Net, ASP.Net, WCF, MVC, Web Services, JQuery, Ajax, Json, AngularJS, Ms Azure and SQL Server and responsible for complete implementation and testing. I have extensive experience in C#, ASP.Net, ADO.Net, WCF, MVC, Web Services, JQuery, Ajax, Angular JS, Ms Azure and SQL Server 2008 RD2 , 12,14. Skills :\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to your text file\n",
    "file_path = \"dataset/UnzippedAngular/Angular Resume.txt\"\n",
    "\n",
    "# Function to read the first n words from the file\n",
    "def read_first_n_words(file_path, n, encoding=\"utf-8\"):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=encoding) as file:\n",
    "            # Read the content of the file\n",
    "            content = file.read()\n",
    "            \n",
    "            # Split the content into words\n",
    "            words = content.split()\n",
    "            \n",
    "            # Take the first n words or less if the file has fewer words\n",
    "            first_n_words = ' '.join(words[:n])\n",
    "            \n",
    "            return first_n_words\n",
    "    except FileNotFoundError:\n",
    "        return \"File not found.\"\n",
    "\n",
    "# Call the function to get the first 100 words\n",
    "first_100_words = read_first_n_words(file_path, 100)\n",
    "\n",
    "# Print the first 100 words\n",
    "print(first_100_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cf34709-e9f7-417d-90d0-7a2192ee5cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset from a text file\n",
    "file_path = \"dataset/UnzippedAngular/Angular Resume.txt\"\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    dataset_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d09c7b3f-7ca7-4144-b988-c0c82c0cd611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56261"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the number of characters in the dataset\n",
    "num_characters = len(dataset_text)\n",
    "num_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f44e5bf6-1249-4337-b237-49cd4e08082b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7401"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the number of words in the dataset\n",
    "words = dataset_text.split()\n",
    "num_words = len(words)\n",
    "num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fa54410-aea9-416a-b329-d27cdf14f5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1302"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the number of lines in the dataset\n",
    "lines = dataset_text.split('\\n')\n",
    "num_lines = len(lines)\n",
    "num_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9005163-f84c-440d-8bf5-bef48ed00380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.862180786380219"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the average word length in the dataset\n",
    "total_word_length = sum(len(word) for word in words)\n",
    "average_word_length = total_word_length / num_words if num_words > 0 else 0\n",
    "average_word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "315a3626-7280-45e8-a994-4cb6243984af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the occurrences of a specific word in the dataset\n",
    "target_word = 'Email'\n",
    "word_count = words.count(target_word)\n",
    "word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60f25b78-d38d-4bef-a28b-926962f4483a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dikesh Ray  \n",
      "Mobile: 7827137507  \n",
      "Email: Dikesh.ray@gmail.com  \n",
      " \n",
      "Subject: Cover Letter – Software Developer  \n",
      " \n",
      "Dear Sir/Madam,  \n",
      " \n",
      "I am MCA degree holder having 5.4 years of experience in analyzing, designing and developing \n",
      "web applications. At present, working with BLS international Services ltd. My primary involvement \n",
      "is in developing web and windows application using C#, ADO.Net, ASP.Net, WCF, MVC, Web \n",
      "Services, JQuery, Ajax, Json, AngularJS, Ms Azure and SQL Server and responsible for complete \n",
      "implementation and testing.  \n",
      "I have extensive experience in C#, ASP.Net, ADO.Net, WCF, MVC, Web Services, JQuery, Ajax, \n",
      "Angular JS, Ms Azure and SQL Server 2008 RD2 , 12,14.  \n",
      " \n",
      "Skills  : \n",
      "● Experience in object oriented design and  development.  \n",
      "● Having good working knowledge of C#, ASP.Net, ADO.Net, WCF, MVC, Web Services, \n",
      "JQuery with Ajax, Json , Angular JS, Ms Azure  etc. \n",
      "● Experience in relational databases design and development ( SQL Server2008 RD2 and \n",
      "2012, 2014 ). \n",
      "● Hav\n"
     ]
    }
   ],
   "source": [
    "# Display the first few lines of the dataset\n",
    "print(dataset_text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c6d1520-ebc8-4dea-8a73-9d5c8e9355dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 words ranked by occurrence:\n",
      "and: 228\n",
      "of: 181\n",
      ":: 128\n",
      "the: 127\n",
      "to: 117\n",
      "in: 89\n",
      "●: 63\n",
      "with: 62\n",
      "project: 57\n",
      "-: 56\n",
      "for: 55\n",
      "web: 54\n",
      "is: 54\n",
      ",: 54\n",
      "software: 48\n",
      "from: 48\n",
      "sql: 46\n",
      "a: 44\n",
      "developer: 41\n",
      "used: 39\n",
      "&: 37\n",
      "on: 36\n",
      "experience: 34\n",
      "as: 34\n",
      "i: 33\n",
      ".: 33\n",
      "server: 31\n",
      "management: 30\n",
      "my: 29\n",
      "application: 28\n",
      "database: 27\n",
      "it: 26\n",
      "javascript,: 24\n",
      ": 24\n",
      "angular: 23\n",
      "by: 23\n",
      "c#,: 22\n",
      "asp.net,: 22\n",
      "work: 22\n",
      "working: 21\n",
      "studio: 21\n",
      "asp.net: 21\n",
      "knowledge: 20\n",
      "tools: 20\n",
      "visual: 20\n",
      "that: 20\n",
      "worked: 20\n",
      "pvt.: 19\n",
      "technologies: 19\n",
      "language: 19\n",
      "ajax: 19\n",
      "environment: 19\n",
      "technology: 19\n",
      "development: 18\n",
      "client: 18\n",
      "windows: 17\n",
      "role: 17\n",
      "user: 17\n",
      "system: 17\n",
      "environment:: 17\n",
      ": 17\n",
      "using: 16\n",
      "ltd: 16\n",
      "based: 16\n",
      "stored: 16\n",
      "have: 15\n",
      "information: 15\n",
      "developed: 15\n",
      "time: 14\n",
      "solutions: 14\n",
      "technical: 14\n",
      "all: 14\n",
      "application.: 14\n",
      "which: 14\n",
      "this: 14\n",
      "2019: 14\n",
      "years: 13\n",
      "skills: 13\n",
      "description:: 13\n",
      "name:: 13\n",
      "write: 13\n",
      "2008: 12\n",
      "good: 12\n",
      "html,: 12\n",
      "css,: 12\n",
      "4: 12\n",
      "responsibilities:: 12\n",
      "can: 12\n",
      "food: 12\n",
      "institute: 12\n",
      "company: 12\n",
      "various: 12\n",
      "create: 12\n",
      "services,: 11\n",
      "design: 11\n",
      "team: 11\n",
      "new: 11\n",
      "java: 11\n",
      "requirement.: 11\n",
      "an: 11\n"
     ]
    }
   ],
   "source": [
    "# Read the dataset from a text file\n",
    "file_path = \"dataset/UnzippedAngular/Angular Resume.txt\"\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    dataset_text = file.read()\n",
    "\n",
    "# Split the text into words\n",
    "words = dataset_text.split()\n",
    "\n",
    "# Create a dictionary to count word occurrences\n",
    "word_counts = {}\n",
    "for word in words:\n",
    "    word = word.lower()  # Convert to lowercase to treat words like \"Word\" and \"word\" as the same\n",
    "    if word in word_counts:\n",
    "        word_counts[word] += 1\n",
    "    else:\n",
    "        word_counts[word] = 1\n",
    "\n",
    "# Sort the dictionary by word count in descending order\n",
    "sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top 20 words by occurrence\n",
    "print(\"Top 100 words ranked by occurrence:\")\n",
    "for word, count in sorted_word_counts[:100]:\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f1644cd-6e87-456a-8489-6ce4e12f626a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-31 01:40:08,231 - INFO - Validation completed with status: True\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataValidationConfig:\n",
    "    root_dir: Path\n",
    "    status_file: Path\n",
    "    unzip_data_dir: Path\n",
    "    expected_elements: list\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self, config_dir):\n",
    "        self.config_dir = config_dir\n",
    "\n",
    "    def get_data_validation_config(self) -> DataValidationConfig:\n",
    "        root_dir = self.config_dir / 'data_validation'\n",
    "        status_file = root_dir / \"validation_status.txt\"\n",
    "        unzip_data_dir = self.config_dir / 'UnzippedAngular'\n",
    "\n",
    "        # List of keywords or patterns expected in the text dataset\n",
    "        expected_elements = [\"Email\", \"experience\", \"language\", \"developed\"]\n",
    "\n",
    "        data_validation_config = DataValidationConfig(\n",
    "            root_dir=root_dir,\n",
    "            status_file=status_file,\n",
    "            unzip_data_dir=unzip_data_dir,\n",
    "            expected_elements=expected_elements\n",
    "        )\n",
    "\n",
    "        return data_validation_config\n",
    "\n",
    "class DataValidation:\n",
    "    def __init__(self, config: DataValidationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def validate_text(self) -> bool:\n",
    "        try:\n",
    "            validation_status = False\n",
    "\n",
    "            # Load your text dataset\n",
    "            dataset_text = self.load_text_dataset()\n",
    "\n",
    "            # Additional analysis\n",
    "            first_100_words = self.read_first_n_words(100)\n",
    "            num_characters = len(dataset_text)\n",
    "            words = dataset_text.split()\n",
    "            num_words = len(words)\n",
    "            lines = dataset_text.split('\\n')\n",
    "            num_lines = len(lines)\n",
    "            average_word_length = sum(len(word) for word in words) / num_words if num_words > 0 else 0\n",
    "            word_count = words.count(\"Email\")\n",
    "            top_words = self.get_top_words(words, 20)\n",
    "\n",
    "            # Check if expected elements are in the dataset text\n",
    "            if all(element in dataset_text for element in self.config.expected_elements):\n",
    "                validation_status = True\n",
    "\n",
    "            # Ensure the directory for the status file exists\n",
    "            self.config.root_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Write results to the status file with utf-8 encoding\n",
    "            with open(self.config.status_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"Validation status: {validation_status}\\n\")\n",
    "\n",
    "            # Write results to the status file\n",
    "            with open(self.config.status_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"Validation status: {validation_status}\\n\")\n",
    "                f.write(f\"First 100 words: {first_100_words}\\n\")\n",
    "                f.write(f\"Total characters: {num_characters}\\n\")\n",
    "                f.write(f\"Total words: {num_words}\\n\")\n",
    "                f.write(f\"Total lines: {num_lines}\\n\")\n",
    "                f.write(f\"Average word length: {average_word_length:.2f}\\n\")\n",
    "                f.write(f\"Occurrences of 'Email': {word_count}\\n\")\n",
    "                f.write(\"Top 20 words ranked by occurrence:\\n\")\n",
    "                for word, count in top_words:\n",
    "                    f.write(f\"{word}: {count}\\n\")\n",
    "\n",
    "            logging.info(f\"Validation completed with status: {validation_status}\")\n",
    "            return validation_status\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during validation: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def read_first_n_words(self, n):\n",
    "        words = self.load_text_dataset().split()\n",
    "        return ' '.join(words[:n])\n",
    "\n",
    "    def get_top_words(self, words, n):\n",
    "        word_counts = {}\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "        return sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "    def load_text_dataset(self):\n",
    "        # Read text file and process it\n",
    "        try:\n",
    "            file_path = self.config.unzip_data_dir / \"Angular Resume.txt\"\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                dataset_text = file.read()\n",
    "            return dataset_text\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading dataset: {e}\")\n",
    "            raise e\n",
    "\n",
    "def main():\n",
    "    # Set config_dir to the absolute path of the dataset directory\n",
    "    config_dir = Path('D:/Desktop/NLP/Lab 1/NLP-Tokenization-and-Language-Modeling/dataset')\n",
    "    config_manager = ConfigurationManager(config_dir)\n",
    "    data_validation_config = config_manager.get_data_validation_config()\n",
    "    data_validation = DataValidation(data_validation_config)\n",
    "    data_validation.validate_text()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7b7eae3-93e0-40d4-813e-9730f9dc40c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation status: True\n",
      "First 100 words: Dikesh Ray Mobile: 7827137507 Email: Dikesh.ray@gmail.com Subject: Cover Letter – Software Developer Dear Sir/Madam, I am MCA degree holder having 5.4 years of experience in analyzing, designing and developing web applications. At present, working with BLS international Services ltd. My primary involvement is in developing web and windows application using C#, ADO.Net, ASP.Net, WCF, MVC, Web Services, JQuery, Ajax, Json, AngularJS, Ms Azure and SQL Server and responsible for complete implementation and testing. I have extensive experience in C#, ASP.Net, ADO.Net, WCF, MVC, Web Services, JQuery, Ajax, Angular JS, Ms Azure and SQL Server 2008 RD2 , 12,14. Skills :\n",
      "Total characters: 56261\n",
      "Total words: 7401\n",
      "Total lines: 1302\n",
      "Average word length: 5.86\n",
      "Occurrences of 'Email': 5\n",
      "Top 20 words ranked by occurrence:\n",
      "and: 228\n",
      "of: 181\n",
      ":: 128\n",
      "the: 127\n",
      "to: 117\n",
      "in: 89\n",
      "●: 63\n",
      "with: 62\n",
      "project: 57\n",
      "-: 56\n",
      "for: 55\n",
      "web: 54\n",
      "is: 54\n",
      ",: 54\n",
      "software: 48\n",
      "from: 48\n",
      "sql: 46\n",
      "a: 44\n",
      "developer: 41\n",
      "used: 39\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the dataset from a text file\n",
    "file_path = \"dataset/data_validation/validation_status.txt\"\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    dataset_text = file.read()\n",
    "\n",
    "print(dataset_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc297008-4329-4050-b551-07f3fc94fe18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150206c9-fa5f-4f43-89df-0a50e4594a72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87046652-b24a-48a0-b8c7-c4077d4c4118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec51cfd-abfa-427d-8df7-836c7ad12532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
